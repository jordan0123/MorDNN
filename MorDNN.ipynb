{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "import sklearn\n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, GRU, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Config Tensorflow to use GPU</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080, pci bus id: 0000:08:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()  \n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  \n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)  \n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.compat.v1.Session(config=config)  \n",
    "tf.compat.v1.keras.backend.set_session(sess)  # set this TensorFlow session as the default session for Keras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 50%\r"
     ]
    }
   ],
   "source": [
    "print(\"progress:%3d%%\" % ((1 / 2) * 100), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create sub-sequences for training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nseq_len = 200 # length of subsequences to train on\\n\\ntrain_x = []\\ntrain_y1 = []\\ntrain_y2 = []\\n\\n# for every full sequence (1 data file)\\nfor i in range(0, len(full_sequences)):\\n    num_samples = len(full_sequences[i]) // 3\\n    \\n    # for each sample in the full sequence, make a subsequence from a window from j to j + seq_len\\n    # num_samples - 1 b/c we are training on the output at the next timestep\\n    for j in range(0, (num_samples - 1 - seq_len) * 3, 3):\\n        subsequence_x = []\\n        subsequence_y1 = []\\n        subsequence_y2 = []\\n        \\n        # for each sample in the window\\n        for k in range(j, j + seq_len * 3, 3):\\n            # game state (input)\\n            subsequence_x.append( np.fromstring(full_sequences[i][k], dtype='float', sep=' ')[0:79] )\\n\\n            # outputs (add 3 b/c each sample is 3 lines and we are training on outputs for the timestep of t + 1)\\n            subsequence_y1.append( np.fromstring(full_sequences[i][k + 1 + 3], dtype='float', sep=' '))\\n            subsequence_y2.append( np.fromstring(full_sequences[i][k + 2 + 3], dtype='float', sep=' '))\\n            \\n        train_x.append(subsequence_x)\\n        train_y1.append(subsequence_y1)\\n        train_y2.append(subsequence_y2)\\n        \\ntrain_x = np.array(train_x)\\ntrain_y1 = np.array(train_y1)\\ntrain_y2 = np.array(train_y2)\\n\\nprint(train_x.shape)\\nprint(train_y1.shape)\\nprint(train_y2.shape)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\Jordan\\\\Desktop\\\\MorDNN\\\\*.txt\"\n",
    "\n",
    "def genDataset(sequence_length):\n",
    "    global seq_len\n",
    "    \n",
    "    global x_train\n",
    "    global y1_train\n",
    "    global y2_train\n",
    "\n",
    "    global num_features\n",
    "    global output_size_y1\n",
    "    global output_size_y2\n",
    "    \n",
    "    full_sequences = []\n",
    "\n",
    "    print('Loading raw data...')\n",
    "    for file in glob.glob(data_path):\n",
    "        data_file = open(file)\n",
    "        full_sequences.append( data_file.readlines() )\n",
    "\n",
    "    num_sequences = len(full_sequences)\n",
    "        \n",
    "    seq_len = sequence_length # length of sequences to train on\n",
    "\n",
    "    data_x = []\n",
    "    data_y1 = []\n",
    "    data_y2 = []\n",
    "\n",
    "    print('Formatting data...')\n",
    "    for sequence in full_sequences:\n",
    "        num_samples = len(sequence) // 3\n",
    "\n",
    "        sequence_x = []\n",
    "        sequence_y1 = []\n",
    "        sequence_y2 = []\n",
    "\n",
    "        # format data into input list and output lists\n",
    "        for i in range (0, (num_samples - 1) * 3, 3):\n",
    "            # align current state with outputs for t+1 \n",
    "            sequence_x.append(np.fromstring(sequence[i], dtype='float', sep=' ')[0:79])\n",
    "            sequence_y1.append(np.fromstring(sequence[i + 1 + 3], dtype='float', sep=' '))\n",
    "            sequence_y2.append(np.fromstring(sequence[i + 2 + 3], dtype='float', sep=' '))\n",
    "\n",
    "        data_x.append(sequence_x)\n",
    "        data_y1.append(sequence_y1)\n",
    "        data_y2.append(sequence_y2)\n",
    "\n",
    "    x_train = []\n",
    "    y1_train = []\n",
    "    y2_train = []\n",
    "\n",
    "    print('Generating training data...')\n",
    "    for i in range(0, len(data_x)):\n",
    "        print(\"progress:%3d%%\" % ((i / num_sequences) * 100), end='\\r')\n",
    "        \n",
    "        num_samples = len(data_x[i])\n",
    "\n",
    "        j = 0\n",
    "        while j < num_samples - seq_len:\n",
    "            subseq_x = []\n",
    "            subseq_y1 = []\n",
    "            subseq_y2 = []\n",
    "\n",
    "            for k in range(j, j + seq_len):\n",
    "                subseq_x.append(data_x[i][k])\n",
    "                subseq_y1.append(data_y1[i][k])\n",
    "                subseq_y2.append(data_y2[i][k])\n",
    "\n",
    "            x_train.append(subseq_x)\n",
    "            y1_train.append(subseq_y1)\n",
    "            y2_train.append(subseq_y2)\n",
    "\n",
    "            j = j + random.randint(1, 2)\n",
    "            \n",
    "    print('Done                       ')\n",
    "\n",
    "    del full_sequences\n",
    "    del data_x\n",
    "    del data_y1\n",
    "    del data_y2\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y1_train = np.array(y1_train)\n",
    "    y2_train = np.array(y2_train)\n",
    "\n",
    "    num_features = x_train[0][0].size\n",
    "    output_size_y1 = y1_train[0][0].size\n",
    "    output_size_y2 = y2_train[0][0].size\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(y1_train.shape)\n",
    "    print(y2_train.shape)\n",
    "    \n",
    "    \n",
    "#genDataset(200)\n",
    "'''\n",
    "seq_len = 200 # length of subsequences to train on\n",
    "\n",
    "train_x = []\n",
    "train_y1 = []\n",
    "train_y2 = []\n",
    "\n",
    "# for every full sequence (1 data file)\n",
    "for i in range(0, len(full_sequences)):\n",
    "    num_samples = len(full_sequences[i]) // 3\n",
    "    \n",
    "    # for each sample in the full sequence, make a subsequence from a window from j to j + seq_len\n",
    "    # num_samples - 1 b/c we are training on the output at the next timestep\n",
    "    for j in range(0, (num_samples - 1 - seq_len) * 3, 3):\n",
    "        subsequence_x = []\n",
    "        subsequence_y1 = []\n",
    "        subsequence_y2 = []\n",
    "        \n",
    "        # for each sample in the window\n",
    "        for k in range(j, j + seq_len * 3, 3):\n",
    "            # game state (input)\n",
    "            subsequence_x.append( np.fromstring(full_sequences[i][k], dtype='float', sep=' ')[0:79] )\n",
    "\n",
    "            # outputs (add 3 b/c each sample is 3 lines and we are training on outputs for the timestep of t + 1)\n",
    "            subsequence_y1.append( np.fromstring(full_sequences[i][k + 1 + 3], dtype='float', sep=' '))\n",
    "            subsequence_y2.append( np.fromstring(full_sequences[i][k + 2 + 3], dtype='float', sep=' '))\n",
    "            \n",
    "        train_x.append(subsequence_x)\n",
    "        train_y1.append(subsequence_y1)\n",
    "        train_y2.append(subsequence_y2)\n",
    "        \n",
    "train_x = np.array(train_x)\n",
    "train_y1 = np.array(train_y1)\n",
    "train_y2 = np.array(train_y2)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y1.shape)\n",
    "print(train_y2.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the neural network\n",
    "# the training model is not stateful and trains on 4 second sequences\n",
    "# the predcition model is stateful and the internal state will persist between predictions\n",
    "def createModel(for_training):\n",
    "    if for_training:\n",
    "        layer_input = Input(shape = (seq_len, num_features))\n",
    "        is_stateful = False\n",
    "    else:\n",
    "        layer_input = Input(batch_shape = (1, 1, num_features))\n",
    "        is_stateful = True\n",
    "        \n",
    "    '''layer_hid1 = LSTM(400, stateful=is_stateful, return_sequences=True)(layer_input)\n",
    "    dropout = Dropout(0.2)(layer_hid1)\n",
    "    layer_hid2 = LSTM(400, stateful=is_stateful, return_sequences=True)(dropout)\n",
    "    dropout2 = Dropout(0.2)(layer_hid2)\n",
    "    layer_hid3 = Dense(100)(dropout2)\n",
    "    layer_hid3 = LeakyReLU(alpha=0.05)(layer_hid3)\n",
    "    dropout3 = Dropout(0.2)(layer_hid3)'''\n",
    "    \n",
    "    layer_hid1 = LSTM(600, stateful=is_stateful, return_sequences=True)(layer_input)\n",
    "    dropout = Dropout(0.2)(layer_hid1)\n",
    "    layer_hid2 = LSTM(500, stateful=is_stateful, return_sequences=True)(dropout)\n",
    "    dropout2 = Dropout(0.2)(layer_hid2)\n",
    "    layer_hid3 = Dense(200)(dropout2)\n",
    "    layer_hid3 = LeakyReLU(alpha=0.05)(layer_hid3)\n",
    "    dropout3 = Dropout(0.2)(layer_hid3)\n",
    "    \n",
    "    # button outputs are probabilistic, goal angles are regressional\n",
    "    layer_out1 = Dense(output_size_y1, activation = 'sigmoid', name='layer_out1')(dropout3)\n",
    "    layer_out2 = Dense(output_size_y2, activation = 'linear', name='layer_out2')(dropout3)\n",
    "\n",
    "    model = Model(inputs=layer_input, outputs=[layer_out1, layer_out2])\n",
    "\n",
    "    if for_training:\n",
    "        model.compile(\n",
    "            loss={'layer_out1' : 'binary_crossentropy', 'layer_out2' : 'mean_squared_error'}, \n",
    "            loss_weights = {'layer_out1' : 8, 'layer_out2' : 0.5}, # 8, 0.5\n",
    "            optimizer='adam', \n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#training_model = createModel(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load weights</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x200fdb8f488>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path_load = \"checkpoint_test\\\\cp.ckpt\"\n",
    "training_model.load_weights(checkpoint_path_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Formatting data...\n",
      "Generating training data...\n",
      "Done                       \n",
      "(129301, 111, 79)\n",
      "(129301, 111, 21)\n",
      "(129301, 111, 2)\n",
      "Train on 129301 samples\n",
      "Epoch 1/3\n",
      "129280/129301 [============================>.] - ETA: 0s - loss: 1.9750 - layer_out1_loss: 0.0268 - layer_out2_loss: 3.5213 - layer_out1_accuracy: 0.9900 - layer_out2_accuracy: 0.9810\n",
      "Epoch 00001: saving model to checkpoint_test\\cp.ckpt\n",
      "129301/129301 [==============================] - 87s 672us/sample - loss: 1.9750 - layer_out1_loss: 0.0268 - layer_out2_loss: 3.5215 - layer_out1_accuracy: 0.9900 - layer_out2_accuracy: 0.9810\n",
      "Epoch 2/3\n",
      "129280/129301 [============================>.] - ETA: 0s - loss: 1.9783 - layer_out1_loss: 0.0264 - layer_out2_loss: 3.5340 - layer_out1_accuracy: 0.9901 - layer_out2_accuracy: 0.9810\n",
      "Epoch 00002: saving model to checkpoint_test\\cp.ckpt\n",
      "129301/129301 [==============================] - 82s 636us/sample - loss: 1.9782 - layer_out1_loss: 0.0264 - layer_out2_loss: 3.5330 - layer_out1_accuracy: 0.9901 - layer_out2_accuracy: 0.9810\n",
      "Epoch 3/3\n",
      "129280/129301 [============================>.] - ETA: 0s - loss: 1.9638 - layer_out1_loss: 0.0262 - layer_out2_loss: 3.5086 - layer_out1_accuracy: 0.9902 - layer_out2_accuracy: 0.9811\n",
      "Epoch 00003: saving model to checkpoint_test\\cp.ckpt\n",
      "129301/129301 [==============================] - 81s 629us/sample - loss: 1.9638 - layer_out1_loss: 0.0262 - layer_out2_loss: 3.5088 - layer_out1_accuracy: 0.9902 - layer_out2_accuracy: 0.9811\n",
      "Loading raw data...\n",
      "Formatting data...\n",
      "Generating training data...\n",
      "Done                       \n",
      "(94050, 276, 79)\n",
      "(94050, 276, 21)\n",
      "(94050, 276, 2)\n",
      "Train on 94050 samples\n",
      "Epoch 1/3\n",
      "93952/94050 [============================>.] - ETA: 0s - loss: 1.5412 - layer_out1_loss: 0.0177 - layer_out2_loss: 2.7986 - layer_out1_accuracy: 0.9933 - layer_out2_accuracy: 0.9846\n",
      "Epoch 00001: saving model to checkpoint_test\\cp.ckpt\n",
      "94050/94050 [==============================] - 151s 2ms/sample - loss: 1.5411 - layer_out1_loss: 0.0177 - layer_out2_loss: 2.7985 - layer_out1_accuracy: 0.9933 - layer_out2_accuracy: 0.9846\n",
      "Epoch 2/3\n",
      "93952/94050 [============================>.] - ETA: 0s - loss: 1.5149 - layer_out1_loss: 0.0170 - layer_out2_loss: 2.7576 - layer_out1_accuracy: 0.9936 - layer_out2_accuracy: 0.9847\n",
      "Epoch 00002: saving model to checkpoint_test\\cp.ckpt\n",
      "94050/94050 [==============================] - 155s 2ms/sample - loss: 1.5150 - layer_out1_loss: 0.0170 - layer_out2_loss: 2.7579 - layer_out1_accuracy: 0.9936 - layer_out2_accuracy: 0.9847\n",
      "Epoch 3/3\n",
      "93952/94050 [============================>.] - ETA: 0s - loss: 1.5162 - layer_out1_loss: 0.0169 - layer_out2_loss: 2.7619 - layer_out1_accuracy: 0.9937 - layer_out2_accuracy: 0.9847\n",
      "Epoch 00003: saving model to checkpoint_test\\cp.ckpt\n",
      "94050/94050 [==============================] - 152s 2ms/sample - loss: 1.5161 - layer_out1_loss: 0.0169 - layer_out2_loss: 2.7619 - layer_out1_accuracy: 0.9937 - layer_out2_accuracy: 0.9847\n",
      "Loading raw data...\n",
      "Formatting data...\n",
      "Generating training data...\n",
      "Done                       \n",
      "(77318, 369, 79)\n",
      "(77318, 369, 21)\n",
      "(77318, 369, 2)\n",
      "Train on 77318 samples\n",
      "Epoch 1/3\n",
      "77312/77318 [============================>.] - ETA: 0s - loss: 1.4668 - layer_out1_loss: 0.0155 - layer_out2_loss: 2.6862 - layer_out1_accuracy: 0.9942 - layer_out2_accuracy: 0.9853\n",
      "Epoch 00001: saving model to checkpoint_test\\cp.ckpt\n",
      "77318/77318 [==============================] - 162s 2ms/sample - loss: 1.4669 - layer_out1_loss: 0.0155 - layer_out2_loss: 2.6873 - layer_out1_accuracy: 0.9942 - layer_out2_accuracy: 0.9853\n",
      "Epoch 2/3\n",
      "77312/77318 [============================>.] - ETA: 0s - loss: 1.4602 - layer_out1_loss: 0.0153 - layer_out2_loss: 2.6764 - layer_out1_accuracy: 0.9943 - layer_out2_accuracy: 0.9853\n",
      "Epoch 00002: saving model to checkpoint_test\\cp.ckpt\n",
      "77318/77318 [==============================] - 152s 2ms/sample - loss: 1.4602 - layer_out1_loss: 0.0153 - layer_out2_loss: 2.6758 - layer_out1_accuracy: 0.9943 - layer_out2_accuracy: 0.9853\n",
      "Epoch 3/3\n",
      "77312/77318 [============================>.] - ETA: 0s - loss: 1.4945 - layer_out1_loss: 0.0159 - layer_out2_loss: 2.7345 - layer_out1_accuracy: 0.9941 - layer_out2_accuracy: 0.9851\n",
      "Epoch 00003: saving model to checkpoint_test\\cp.ckpt\n",
      "77318/77318 [==============================] - 155s 2ms/sample - loss: 1.4945 - layer_out1_loss: 0.0159 - layer_out2_loss: 2.7338 - layer_out1_accuracy: 0.9941 - layer_out2_accuracy: 0.9851\n",
      "Loading raw data...\n",
      "Formatting data...\n",
      "Generating training data...\n",
      "Done                       \n",
      "(113314, 180, 79)\n",
      "(113314, 180, 21)\n",
      "(113314, 180, 2)\n",
      "Train on 113314 samples\n",
      "Epoch 1/3\n",
      " 22016/113314 [====>.........................] - ETA: 1:37 - loss: 1.6861 - layer_out1_loss: 0.0210 - layer_out2_loss: 3.0364 - layer_out1_accuracy: 0.9923 - layer_out2_accuracy: 0.9833"
     ]
    }
   ],
   "source": [
    "checkpoint_path_load = \"checkpoint_test\\\\cp.ckpt\"\n",
    "checkpoint_path_save = \"checkpoint_test\\\\cp.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_save,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    genDataset(random.randint(100, 400))\n",
    "    \n",
    "    training_model = createModel(True)\n",
    "    training_model.load_weights(checkpoint_path_load)\n",
    "    \n",
    "    history = training_model.fit(x_train, {'layer_out1' : y1_train, 'layer_out2' : y2_train},\n",
    "                    epochs=3, batch_size = 128, shuffle=True,\n",
    "                    callbacks=[cp_callback])\n",
    "    \n",
    "    checkpoint_path_load = checkpoint_path_save\n",
    "    \n",
    "    del x_train\n",
    "    del y1_train\n",
    "    del y2_train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103228 samples\n",
      "Epoch 1/500\n",
      "103168/103228 [============================>.] - ETA: 0s - loss: 1.5284 - layer_out1_loss: 0.0166 - layer_out2_loss: 2.7908 - layer_out1_accuracy: 0.9938 - layer_out2_accuracy: 0.9845\n",
      "Epoch 00001: saving model to checkpoint_test\\cp.ckpt\n",
      "103228/103228 [==============================] - 131s 1ms/sample - loss: 1.5285 - layer_out1_loss: 0.0166 - layer_out2_loss: 2.7910 - layer_out1_accuracy: 0.9938 - layer_out2_accuracy: 0.9845\n",
      "Epoch 2/500\n",
      "  2304/103228 [..............................] - ETA: 1:56 - loss: 1.4893 - layer_out1_loss: 0.0164 - layer_out2_loss: 2.7170 - layer_out1_accuracy: 0.9939 - layer_out2_accuracy: 0.9840\n",
      "Epoch 00002: saving model to checkpoint_test\\cp.ckpt\n",
      "  2304/103228 [..............................] - ETA: 2:01 - loss: 1.4893 - layer_out1_loss: 0.0164 - layer_out2_loss: 2.7170 - layer_out1_accuracy: 0.9939 - layer_out2_accuracy: 0.9840"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-872a04324e70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m history = training_model.fit(x_train, {'layer_out1' : y1_train, 'layer_out2' : y2_train},\n\u001b[0;32m      9\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     callbacks=[cp_callback])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mine\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_path_save = \"checkpoint_test\\\\cp.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_save,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history = training_model.fit(x_train, {'layer_out1' : y1_train, 'layer_out2' : y2_train},\n",
    "                    epochs=500, batch_size = 128, shuffle=True,\n",
    "                    callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create and save stateful prediction model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(1, 1, 79)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (1, 1, 600)          1632000     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (1, 1, 600)          0           lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  (1, 1, 500)          2202000     dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (1, 1, 500)          0           lstm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (1, 1, 200)          100200      dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (1, 1, 200)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (1, 1, 200)          0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_out1 (Dense)              (1, 1, 21)           4221        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_out2 (Dense)              (1, 1, 2)            402         dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,938,823\n",
      "Trainable params: 3,938,823\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prediction_model = createModel(False)\n",
    "prediction_model.load_weights(checkpoint_path_save)\n",
    "prediction_model.save('mordhai.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
